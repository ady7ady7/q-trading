{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DAX Local Pivot Conditional Probabilities by Volatility Regime\n",
    "## Does First-Hour Volatility Change Pivot Edge Strength?\n",
    "\n",
    "**Research Question:**\n",
    "Do local pivot conditional probabilities vary by volatility regime?\n",
    "- **Hypothesis 1:** Quiet markets (Q1) → stronger mean reversion to local pivot levels\n",
    "- **Hypothesis 2:** Spicy markets (Q4) → weaker pivot support/resistance, more breakouts\n",
    "- **Goal:** Find regime-specific conditional edges\n",
    "\n",
    "**Methodology:**\n",
    "1. **Calculate Local Pivots** from first hour (9:00-10:00) H/L/C\n",
    "2. **Calculate First Hour Volatility Regime:**\n",
    "   - Early_TR = Sum of True Range across first 12 M5 bars (9:00-9:55)\n",
    "   - Baseline = 20-day average of Early_TR (shifted, no look-ahead)\n",
    "   - Early_Ratio = Early_TR / Baseline\n",
    "   - **Outlier trimming:** Remove top 5% and bottom 5% extremes\n",
    "   - **Dynamic quartiles:** Rolling 60-day percentile rank on trimmed data\n",
    "   - Q1 (Quiet) = 0-25th percentile, Q2 = 25-50th, Q3 = 50-75th, Q4 (Spicy) = 75-100th\n",
    "\n",
    "3. **Classify Opening Zone at 10:00** relative to local pivots\n",
    "4. **Track First Touch Timestamps** (10:00-17:30 session)\n",
    "5. **Calculate Conditional Probabilities BY REGIME:**\n",
    "   - P(Target | Condition, Zone, Regime)\n",
    "   - Temporal + Spatial ordering enforced\n",
    "   - Compare: Does Q1 (quiet) vs Q4 (spicy) show different conditional probabilities?\n",
    "\n",
    "**Key Innovation:**\n",
    "- Previous research: Local pivots conditional probabilities (all regimes combined)\n",
    "- This research: SPLIT by volatility regime to find regime-specific edges\n",
    "\n",
    "**Expected Insights:**\n",
    "- If Q1 shows higher mean-reversion probabilities → trade reversals in quiet markets\n",
    "- If Q4 shows lower pivot hold probabilities → avoid fading breakouts in spicy markets\n",
    "- If no difference → regime doesn't matter for local pivots (use combined probabilities)\n",
    "\n",
    "**Data:** M5 OHLCV, Jan 2021 - present, RTH (09:00-17:30 Berlin)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Dependencies loaded\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../../')\n",
    "\n",
    "from shared.database_connector import fetch_ohlcv, get_date_range\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (22, 16)\n",
    "\n",
    "print('[OK] Dependencies loaded')\n",
    "print('='*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Fetch M5 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-18 22:05:23,318 - shared.database_connector - INFO - Initializing database connection...\n",
      "2025-12-18 22:05:23,319 - shared.database_connector - WARNING - CA certificate not found at certs\\ca-certificate.crt. Connecting without SSL verification.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP 2] Fetch M5 Data\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-18 22:05:24,114 - shared.database_connector - INFO - [OK] Database connection successful\n",
      "2025-12-18 22:05:24,347 - shared.database_connector - INFO - [OK] Date range for deuidxeur m5: 2020-09-14 22:00:00+00:00 to 2025-12-11 22:55:00+00:00\n",
      "2025-12-18 22:05:24,349 - shared.database_connector - INFO - fetch_ohlcv(): symbol=deuidxeur, timeframe=m5, start=2021-01-01 00:00:00, end=2025-12-11 22:55:00+00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching M5 data: 2021-01-01 to 2025-12-11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-18 22:05:27,847 - shared.database_connector - INFO - [OK] Fetched 339450 candles (2021-01-03 22:00:00+00:00 to 2025-12-11 22:55:00+00:00)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Fetched 339450 M5 candles\n",
      "[OK] RTH filtered: 131382 candles\n"
     ]
    }
   ],
   "source": [
    "print('\\n[STEP 2] Fetch M5 Data')\n",
    "print('='*80)\n",
    "\n",
    "date_range = get_date_range('deuidxeur', 'm5')\n",
    "end_date = date_range['end']\n",
    "start_date = datetime(2021, 1, 1)\n",
    "\n",
    "print(f'Fetching M5 data: {start_date.date()} to {end_date.date()}')\n",
    "\n",
    "df_raw = fetch_ohlcv(\n",
    "    symbol='deuidxeur',\n",
    "    timeframe='m5',\n",
    "    start_date=start_date,\n",
    "    end_date=end_date\n",
    ")\n",
    "\n",
    "df_m5 = df_raw.copy()\n",
    "df_m5.index = df_m5.index.tz_convert('Europe/Berlin')\n",
    "\n",
    "print(f'[OK] Fetched {len(df_m5)} M5 candles')\n",
    "\n",
    "df_m5['date'] = df_m5.index.date\n",
    "df_m5['hour'] = df_m5.index.hour\n",
    "df_m5['minute'] = df_m5.index.minute\n",
    "\n",
    "df_m5_rth = df_m5[\n",
    "    (df_m5['hour'] >= 9) & \n",
    "    ((df_m5['hour'] < 17) | ((df_m5['hour'] == 17) & (df_m5['minute'] <= 30)))\n",
    "].copy()\n",
    "\n",
    "print(f'[OK] RTH filtered: {len(df_m5_rth)} candles')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Calculate Local Pivots AND Volatility Regime\n",
    "\n",
    "**Why combine these steps?**\n",
    "Both use the same first hour data (9:00-10:00), so we calculate them together efficiently.\n",
    "\n",
    "**Volatility Regime Calculation:**\n",
    "1. True Range (TR) = max(H-L, |H-C_prev|, |L-C_prev|) for each M5 bar\n",
    "2. Early_TR = Sum of TR across first 12 bars (9:00-9:55)\n",
    "3. Baseline = 20-day average Early_TR (shifted to avoid look-ahead)\n",
    "4. Early_Ratio = Early_TR / Baseline\n",
    "5. Outlier trimming: Remove top 5% and bottom 5%\n",
    "6. Rolling 60-day percentile rank on trimmed data\n",
    "7. Quartile assignment: Q1 (0-25%), Q2 (25-50%), Q3 (50-75%), Q4 (75-100%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP 3] Calculate Local Pivots AND Volatility Regime\n",
      "================================================================================\n",
      "[OK] Calculated local pivots and Early_TR for 1276 days\n",
      "[OK] Calculated Early_Ratio: 1266 valid days\n",
      "[OK] Outlier trimming: clipped at 0.60 (5th percentile) and 1.67 (95th percentile)\n",
      "[OK] Volatility regime assigned: 1237 days\n",
      "\n",
      "Regime distribution:\n",
      "  Q4_Spicy      322 days ( 26.0%)\n",
      "  Q2_Neutral    322 days ( 26.0%)\n",
      "  Q1_Quiet      297 days ( 24.0%)\n",
      "  Q3_Neutral    296 days ( 23.9%)\n"
     ]
    }
   ],
   "source": [
    "print('\\n[STEP 3] Calculate Local Pivots AND Volatility Regime')\n",
    "print('='*80)\n",
    "\n",
    "df_first_hour = df_m5_rth[\n",
    "    (df_m5_rth['hour'] == 9) & (df_m5_rth['minute'] < 60)\n",
    "].copy()\n",
    "\n",
    "# Calculate True Range for each M5 bar\n",
    "df_first_hour['prev_close'] = df_first_hour.groupby('date')['close'].shift(1)\n",
    "df_first_hour['tr'] = df_first_hour.apply(\n",
    "    lambda row: max(\n",
    "        row['high'] - row['low'],\n",
    "        abs(row['high'] - row['prev_close']) if pd.notna(row['prev_close']) else row['high'] - row['low'],\n",
    "        abs(row['low'] - row['prev_close']) if pd.notna(row['prev_close']) else row['high'] - row['low']\n",
    "    ),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "daily_data = []\n",
    "\n",
    "for date, day_bars in df_first_hour.groupby('date'):\n",
    "    if len(day_bars) < 10:\n",
    "        continue\n",
    "    \n",
    "    # Local Pivot calculation\n",
    "    H = day_bars['high'].max()\n",
    "    L = day_bars['low'].min()\n",
    "    C = day_bars.iloc[-1]['close']\n",
    "    \n",
    "    LPP = (H + L + C) / 3\n",
    "    LR1 = (2 * LPP) - L\n",
    "    LS1 = (2 * LPP) - H\n",
    "    LR2 = LPP + (H - L)\n",
    "    LS2 = LPP - (H - L)\n",
    "    LR3 = H + 2 * (LPP - L)\n",
    "    LS3 = L - 2 * (H - LPP)\n",
    "    \n",
    "    # Early Volatility calculation\n",
    "    Early_TR = day_bars['tr'].sum()\n",
    "    \n",
    "    daily_data.append({\n",
    "        'date': date,\n",
    "        'first_hour_high': H,\n",
    "        'first_hour_low': L,\n",
    "        'first_hour_close': C,\n",
    "        'LPP': LPP,\n",
    "        'LR1': LR1,\n",
    "        'LS1': LS1,\n",
    "        'LR2': LR2,\n",
    "        'LS2': LS2,\n",
    "        'LR3': LR3,\n",
    "        'LS3': LS3,\n",
    "        'Early_TR': Early_TR,\n",
    "    })\n",
    "\n",
    "df_daily = pd.DataFrame(daily_data).sort_values('date').reset_index(drop=True)\n",
    "\n",
    "print(f'[OK] Calculated local pivots and Early_TR for {len(df_daily)} days')\n",
    "\n",
    "# Calculate baseline (20-day average, shifted)\n",
    "df_daily['Baseline'] = df_daily['Early_TR'].rolling(20, min_periods=10).mean().shift(1)\n",
    "\n",
    "# Drop first days without baseline\n",
    "df_daily = df_daily[df_daily['Baseline'].notna()].reset_index(drop=True)\n",
    "\n",
    "# Calculate Early_Ratio\n",
    "df_daily['Early_Ratio'] = df_daily['Early_TR'] / df_daily['Baseline']\n",
    "\n",
    "print(f'[OK] Calculated Early_Ratio: {len(df_daily)} valid days')\n",
    "\n",
    "# Outlier trimming (remove top 5% and bottom 5%)\n",
    "p5 = df_daily['Early_Ratio'].quantile(0.05)\n",
    "p95 = df_daily['Early_Ratio'].quantile(0.95)\n",
    "\n",
    "df_daily['Early_Ratio_Trimmed'] = df_daily['Early_Ratio'].clip(lower=p5, upper=p95)\n",
    "\n",
    "print(f'[OK] Outlier trimming: clipped at {p5:.2f} (5th percentile) and {p95:.2f} (95th percentile)')\n",
    "\n",
    "# Rolling 60-day percentile rank (on trimmed data)\n",
    "df_daily['Early_Percentile'] = df_daily['Early_Ratio_Trimmed'].rolling(60, min_periods=30).apply(\n",
    "    lambda x: pd.Series(x).rank(pct=True).iloc[-1] if len(x) > 0 else np.nan\n",
    ")\n",
    "\n",
    "# Quartile assignment\n",
    "def assign_quartile(percentile):\n",
    "    if pd.isna(percentile):\n",
    "        return None\n",
    "    if percentile <= 0.25:\n",
    "        return 'Q1_Quiet'\n",
    "    elif percentile <= 0.50:\n",
    "        return 'Q2_Neutral'\n",
    "    elif percentile <= 0.75:\n",
    "        return 'Q3_Neutral'\n",
    "    else:\n",
    "        return 'Q4_Spicy'\n",
    "\n",
    "df_daily['Regime'] = df_daily['Early_Percentile'].apply(assign_quartile)\n",
    "\n",
    "# Drop days without regime\n",
    "df_daily = df_daily[df_daily['Regime'].notna()].reset_index(drop=True)\n",
    "\n",
    "print(f'[OK] Volatility regime assigned: {len(df_daily)} days')\n",
    "print(f'\\nRegime distribution:')\n",
    "for regime, count in df_daily['Regime'].value_counts().items():\n",
    "    pct = count / len(df_daily) * 100\n",
    "    print(f'  {regime:12} {count:4d} days ({pct:5.1f}%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Get 10:00 Price and Classify Zone\n",
    "\n",
    "**Why 10:00?**\n",
    "Local pivots are calculated from 9:00-10:00 range, so we need to wait until 10:00 to classify the opening zone and start tracking conditional probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP 4] Get 10:00 Price and Classify Zone\n",
      "================================================================================\n",
      "[OK] Classified opening zones: 1237 days\n",
      "\n",
      "Zone distribution:\n",
      "  LPP_LR1       680 days\n",
      "  LS1_LPP       557 days\n"
     ]
    }
   ],
   "source": [
    "print('\\n[STEP 4] Get 10:00 Price and Classify Zone')\n",
    "print('='*80)\n",
    "\n",
    "df_10am = df_m5_rth[\n",
    "    (df_m5_rth['hour'] == 10) & (df_m5_rth['minute'] == 0)\n",
    "][['date', 'open']].copy()\n",
    "\n",
    "df_10am = df_10am.rename(columns={'open': 'price_10am'})\n",
    "df_daily = df_daily.merge(df_10am, on='date', how='left')\n",
    "df_daily = df_daily[df_daily['price_10am'].notna()].reset_index(drop=True)\n",
    "\n",
    "def classify_local_zone(row):\n",
    "    price = row['price_10am']\n",
    "    if price > row['LR3']:\n",
    "        return 'Above_LR3'\n",
    "    elif row['LR2'] < price <= row['LR3']:\n",
    "        return 'LR2_LR3'\n",
    "    elif row['LR1'] < price <= row['LR2']:\n",
    "        return 'LR1_LR2'\n",
    "    elif row['LPP'] < price <= row['LR1']:\n",
    "        return 'LPP_LR1'\n",
    "    elif row['LS1'] < price <= row['LPP']:\n",
    "        return 'LS1_LPP'\n",
    "    elif row['LS2'] < price <= row['LS1']:\n",
    "        return 'LS2_LS1'\n",
    "    elif row['LS3'] < price <= row['LS2']:\n",
    "        return 'LS3_LS2'\n",
    "    else:\n",
    "        return 'Below_LS3'\n",
    "\n",
    "df_daily['opening_zone'] = df_daily.apply(classify_local_zone, axis=1)\n",
    "\n",
    "print(f'[OK] Classified opening zones: {len(df_daily)} days')\n",
    "print(f'\\nZone distribution:')\n",
    "for zone, count in df_daily['opening_zone'].value_counts().items():\n",
    "    print(f'  {zone:12} {count:4d} days')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Define Target Levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP 5] Define Target Levels\n",
      "================================================================================\n",
      "[OK] Target levels calculated\n"
     ]
    }
   ],
   "source": [
    "print('\\n[STEP 5] Define Target Levels')\n",
    "print('='*80)\n",
    "\n",
    "def calculate_local_targets(row):\n",
    "    targets = {}\n",
    "    \n",
    "    targets['LS3'] = row['LS3']\n",
    "    targets['LS2'] = row['LS2']\n",
    "    targets['LS1'] = row['LS1']\n",
    "    targets['LPP'] = row['LPP']\n",
    "    targets['LR1'] = row['LR1']\n",
    "    targets['LR2'] = row['LR2']\n",
    "    targets['LR3'] = row['LR3']\n",
    "    \n",
    "    for low_level, high_level, prefix in [\n",
    "        ('LS3', 'LS2', 'LS2_LS3'),\n",
    "        ('LS2', 'LS1', 'LS1_LS2'),\n",
    "        ('LS1', 'LPP', 'LS1_LPP'),\n",
    "        ('LPP', 'LR1', 'LPP_LR1'),\n",
    "        ('LR1', 'LR2', 'LR1_LR2'),\n",
    "        ('LR2', 'LR3', 'LR2_LR3'),\n",
    "    ]:\n",
    "        low_val = row[low_level]\n",
    "        high_val = row[high_level]\n",
    "        dist = high_val - low_val\n",
    "        targets[f'{prefix}_025'] = low_val + 0.25 * dist\n",
    "        targets[f'{prefix}_050'] = low_val + 0.50 * dist\n",
    "        targets[f'{prefix}_075'] = low_val + 0.75 * dist\n",
    "    \n",
    "    return targets\n",
    "\n",
    "df_daily['targets'] = df_daily.apply(calculate_local_targets, axis=1)\n",
    "\n",
    "print(f'[OK] Target levels calculated')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Track First Touch Timestamps (10:00-17:30 Session)\n",
    "\n",
    "**Why track timestamps?**\n",
    "To enforce temporal ordering - we only count a target as reached AFTER the condition level was hit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP 6] Track First Touch Timestamps\n",
      "================================================================================\n",
      "[WARNING] Processing intrabar data - may take 2-3 minutes...\n",
      "  Processed 200/1237 days...\n",
      "  Processed 400/1237 days...\n",
      "  Processed 600/1237 days...\n",
      "  Processed 800/1237 days...\n",
      "  Processed 1000/1237 days...\n",
      "  Processed 1200/1237 days...\n",
      "\n",
      "[OK] First touch timestamps tracked\n"
     ]
    }
   ],
   "source": [
    "print('\\n[STEP 6] Track First Touch Timestamps')\n",
    "print('='*80)\n",
    "print('[WARNING] Processing intrabar data - may take 2-3 minutes...')\n",
    "\n",
    "df_m5_session = df_m5_rth[df_m5_rth['hour'] >= 10].copy()\n",
    "\n",
    "def track_first_touches_session(date, targets_dict, df_m5_day):\n",
    "    first_touches = {key: None for key in targets_dict.keys()}\n",
    "    \n",
    "    for timestamp, bar in df_m5_day.iterrows():\n",
    "        bar_high = bar['high']\n",
    "        bar_low = bar['low']\n",
    "        \n",
    "        for target_name, target_price in targets_dict.items():\n",
    "            if first_touches[target_name] is not None:\n",
    "                continue\n",
    "            \n",
    "            if bar_low <= target_price <= bar_high:\n",
    "                first_touches[target_name] = timestamp\n",
    "    \n",
    "    return first_touches\n",
    "\n",
    "first_touch_data = []\n",
    "\n",
    "for idx, day_row in df_daily.iterrows():\n",
    "    date = day_row['date']\n",
    "    targets_dict = day_row['targets']\n",
    "    \n",
    "    df_m5_day = df_m5_session[df_m5_session['date'] == date].copy()\n",
    "    \n",
    "    if len(df_m5_day) < 10:\n",
    "        continue\n",
    "    \n",
    "    first_touches = track_first_touches_session(date, targets_dict, df_m5_day)\n",
    "    \n",
    "    first_touch_data.append({\n",
    "        'date': date,\n",
    "        'first_touches': first_touches\n",
    "    })\n",
    "    \n",
    "    if (idx + 1) % 200 == 0:\n",
    "        print(f'  Processed {idx + 1}/{len(df_daily)} days...')\n",
    "\n",
    "df_touch = pd.DataFrame(first_touch_data)\n",
    "df_daily = df_daily.merge(df_touch, on='date', how='left')\n",
    "\n",
    "print(f'\\n[OK] First touch timestamps tracked')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Calculate Conditional Probabilities BY REGIME (Spatial + Temporal)\n",
    "\n",
    "**Key Innovation:**\n",
    "We split the conditional probability calculation by volatility regime.\n",
    "\n",
    "**Why this matters:**\n",
    "- In Q1 (Quiet): Local pivots may act as strong support/resistance → higher mean reversion probabilities\n",
    "- In Q4 (Spicy): Local pivots may break more easily → lower hold probabilities, higher breakout probabilities\n",
    "\n",
    "**Spatial + Temporal Ordering:**\n",
    "To count a target as reached after condition, we check:\n",
    "1. **Temporal:** timestamp_target > timestamp_condition\n",
    "2. **Spatial:** ALL intermediate levels between condition and target were reached\n",
    "\n",
    "This eliminates false positives from price gaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP 7] Calculate Conditional Probabilities BY REGIME\n",
      "================================================================================\n",
      "[OK] Conditional probability function defined (regime-aware)\n"
     ]
    }
   ],
   "source": [
    "print('\\n[STEP 7] Calculate Conditional Probabilities BY REGIME')\n",
    "print('='*80)\n",
    "\n",
    "LEVEL_ORDER = [\n",
    "    'LS3',\n",
    "    'LS2_LS3_025', 'LS2_LS3_050', 'LS2_LS3_075',\n",
    "    'LS2',\n",
    "    'LS1_LS2_025', 'LS1_LS2_050', 'LS1_LS2_075',\n",
    "    'LS1',\n",
    "    'LS1_LPP_025', 'LS1_LPP_050', 'LS1_LPP_075',\n",
    "    'LPP',\n",
    "    'LPP_LR1_025', 'LPP_LR1_050', 'LPP_LR1_075',\n",
    "    'LR1',\n",
    "    'LR1_LR2_025', 'LR1_LR2_050', 'LR1_LR2_075',\n",
    "    'LR2',\n",
    "    'LR2_LR3_025', 'LR2_LR3_050', 'LR2_LR3_075',\n",
    "    'LR3',\n",
    "]\n",
    "\n",
    "def get_intermediate_levels(condition, target, level_order):\n",
    "    try:\n",
    "        cond_idx = level_order.index(condition)\n",
    "        target_idx = level_order.index(target)\n",
    "    except ValueError:\n",
    "        return []\n",
    "    \n",
    "    if cond_idx == target_idx:\n",
    "        return []\n",
    "    \n",
    "    if cond_idx < target_idx:\n",
    "        return level_order[cond_idx + 1:target_idx + 1]\n",
    "    else:\n",
    "        return level_order[target_idx:cond_idx][::-1]\n",
    "\n",
    "def calculate_conditional_probs_by_regime(df, zone_name, condition_level, regime_filter):\n",
    "    \"\"\"\n",
    "    Calculate P(Target | Condition, Zone, Regime) with spatial + temporal ordering.\n",
    "    \n",
    "    Args:\n",
    "        regime_filter: 'Q1_Quiet', 'Q2_Neutral', 'Q3_Neutral', 'Q4_Spicy', or None (all regimes)\n",
    "    \"\"\"\n",
    "    zone_data = df[df['opening_zone'] == zone_name].copy()\n",
    "    \n",
    "    # Filter by regime if specified\n",
    "    if regime_filter is not None:\n",
    "        zone_data = zone_data[zone_data['Regime'] == regime_filter].copy()\n",
    "    \n",
    "    if len(zone_data) < 10:\n",
    "        return None\n",
    "    \n",
    "    zone_data['condition_timestamp'] = zone_data['first_touches'].apply(\n",
    "        lambda ft: ft.get(condition_level) if ft is not None else None\n",
    "    )\n",
    "    \n",
    "    condition_met = zone_data[zone_data['condition_timestamp'].notna()].copy()\n",
    "    n_condition = len(condition_met)\n",
    "    \n",
    "    if n_condition < 5:\n",
    "        return None\n",
    "    \n",
    "    target_keys = list(condition_met.iloc[0]['targets'].keys())\n",
    "    \n",
    "    results = []\n",
    "    for target_key in target_keys:\n",
    "        n_sequential_spatial = 0\n",
    "        \n",
    "        intermediates = get_intermediate_levels(condition_level, target_key, LEVEL_ORDER)\n",
    "        \n",
    "        for _, row in condition_met.iterrows():\n",
    "            condition_ts = row['condition_timestamp']\n",
    "            target_ts = row['first_touches'].get(target_key) if row['first_touches'] is not None else None\n",
    "            \n",
    "            if target_ts is None or condition_ts is None:\n",
    "                continue\n",
    "            \n",
    "            if target_ts <= condition_ts:\n",
    "                continue\n",
    "            \n",
    "            all_intermediates_reached = True\n",
    "            for inter_level in intermediates:\n",
    "                inter_ts = row['first_touches'].get(inter_level) if row['first_touches'] is not None else None\n",
    "                if inter_ts is None:\n",
    "                    all_intermediates_reached = False\n",
    "                    break\n",
    "            \n",
    "            if all_intermediates_reached:\n",
    "                n_sequential_spatial += 1\n",
    "        \n",
    "        prob_conditional = n_sequential_spatial / n_condition if n_condition > 0 else 0\n",
    "        \n",
    "        results.append({\n",
    "            'target': target_key,\n",
    "            'count_sequential': n_sequential_spatial,\n",
    "            'prob_conditional': prob_conditional,\n",
    "            'n_condition': n_condition,\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "print('[OK] Conditional probability function defined (regime-aware)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Analyze Key Zones - Compare Regimes\n",
    "\n",
    "**Goal:** For each zone + condition, compare conditional probabilities across regimes.\n",
    "\n",
    "**Key Question:** Does Q1 (Quiet) show different probabilities than Q4 (Spicy)?\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "Zone: LPP_LR1, Condition: LR1\n",
    "\n",
    "Q1 (Quiet):  P(LPP | LR1) = 75% (strong mean reversion)\n",
    "Q4 (Spicy):  P(LPP | LR1) = 45% (weak mean reversion, price tends to break)\n",
    "\n",
    "Delta: +30% → In quiet markets, LR1 acts as resistance and price reverts to LPP\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP 8] Analyze Key Zones - Compare Across Regimes\n",
      "================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "ZONE: LPP_LR1 (N = 680 days total)\n",
      "====================================================================================================\n",
      "\n",
      "  CONDITION: LPP\n",
      "  -----------------------------------------------------------------------------------------------\n",
      "\n",
      "  CONDITION: LPP_LR1_050\n",
      "  -----------------------------------------------------------------------------------------------\n",
      "\n",
      "  CONDITION: LR1\n",
      "  -----------------------------------------------------------------------------------------------\n",
      "\n",
      "  Target                 Q1 (Quiet)           Q2           Q3   Q4 (Spicy)          ALL     Q1-Q4 Delta\n",
      "  --------------------------------------------------------------------------------------------------------------\n",
      "  LPP                           20%          28%          32%          16%          24%             +4%\n",
      "  LPP_LR1_050                    1%           0%           0%           0%           0%             +1%\n",
      "  LR1_LR2_050                   73%          70%          77%          82%          75%             -9%\n",
      "  LR2                           74%          73%          75%          77%          75%             -3%\n",
      "\n",
      "  Sample sizes (N days where condition reached):\n",
      "    Q1_Quiet     N = 157\n",
      "    Q2_Neutral   N = 148\n",
      "    Q3_Neutral   N = 122\n",
      "    Q4_Spicy     N = 125\n",
      "    ALL          N = 552\n",
      "\n",
      "====================================================================================================\n",
      "ZONE: LS1_LPP (N = 557 days total)\n",
      "====================================================================================================\n",
      "\n",
      "  CONDITION: LS1\n",
      "  -----------------------------------------------------------------------------------------------\n",
      "\n",
      "  Target                 Q1 (Quiet)           Q2           Q3   Q4 (Spicy)          ALL     Q1-Q4 Delta\n",
      "  --------------------------------------------------------------------------------------------------------------\n",
      "  LS2                           67%          68%          79%          71%          71%             -5%\n",
      "  LS1_LS2_050                   62%          61%          66%          77%          67%            -15%\n",
      "  LS1_LPP_050                    0%           0%           0%           0%           0%             +0%\n",
      "  LPP                           23%          23%          17%          24%          22%             -0%\n",
      "\n",
      "  Sample sizes (N days where condition reached):\n",
      "    Q1_Quiet     N = 90\n",
      "    Q2_Neutral   N = 120\n",
      "    Q3_Neutral   N = 108\n",
      "    Q4_Spicy     N = 122\n",
      "    ALL          N = 440\n",
      "\n",
      "  CONDITION: LS1_LPP_050\n",
      "  -----------------------------------------------------------------------------------------------\n",
      "\n",
      "  CONDITION: LPP\n",
      "  -----------------------------------------------------------------------------------------------\n",
      "\n",
      "====================================================================================================\n",
      "[OK] Regime comparison complete\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "print('\\n[STEP 8] Analyze Key Zones - Compare Across Regimes')\n",
    "print('='*80)\n",
    "\n",
    "# Focus on most common zones\n",
    "key_zones = ['LPP_LR1', 'LS1_LPP']\n",
    "key_conditions = {\n",
    "    'LPP_LR1': ['LPP', 'LPP_LR1_050', 'LR1'],\n",
    "    'LS1_LPP': ['LS1', 'LS1_LPP_050', 'LPP'],\n",
    "}\n",
    "\n",
    "# Key targets to compare (mean reversion vs continuation)\n",
    "key_targets = {\n",
    "    'LPP_LR1': {\n",
    "        'LR1': ['LPP', 'LPP_LR1_050', 'LR1_LR2_050', 'LR2'],  # Reversion vs breakout\n",
    "    },\n",
    "    'LS1_LPP': {\n",
    "        'LS1': ['LS2', 'LS1_LS2_050', 'LS1_LPP_050', 'LPP'],  # Reversion vs breakdown\n",
    "    },\n",
    "}\n",
    "\n",
    "all_results = {}\n",
    "\n",
    "for zone in key_zones:\n",
    "    if zone not in df_daily['opening_zone'].values:\n",
    "        continue\n",
    "    \n",
    "    zone_data = df_daily[df_daily['opening_zone'] == zone]\n",
    "    n_zone = len(zone_data)\n",
    "    \n",
    "    print(f'\\n{\"=\"*100}')\n",
    "    print(f'ZONE: {zone} (N = {n_zone} days total)')\n",
    "    print(f'{\"=\"*100}')\n",
    "    \n",
    "    all_results[zone] = {}\n",
    "    \n",
    "    for condition in key_conditions[zone]:\n",
    "        print(f'\\n  CONDITION: {condition}')\n",
    "        print(f'  {\"-\"*95}')\n",
    "        \n",
    "        # Calculate for each regime\n",
    "        regime_results = {}\n",
    "        \n",
    "        for regime in ['Q1_Quiet', 'Q2_Neutral', 'Q3_Neutral', 'Q4_Spicy']:\n",
    "            df_regime = calculate_conditional_probs_by_regime(df_daily, zone, condition, regime)\n",
    "            if df_regime is not None:\n",
    "                regime_results[regime] = df_regime\n",
    "        \n",
    "        # Also calculate for ALL regimes combined (baseline)\n",
    "        df_all = calculate_conditional_probs_by_regime(df_daily, zone, condition, None)\n",
    "        if df_all is not None:\n",
    "            regime_results['ALL'] = df_all\n",
    "        \n",
    "        all_results[zone][condition] = regime_results\n",
    "        \n",
    "        # Display comparison for key targets\n",
    "        if condition in key_targets.get(zone, {}):\n",
    "            targets_to_show = key_targets[zone][condition]\n",
    "            \n",
    "            print(f'\\n  {\"Target\":<20} {\"Q1 (Quiet)\":>12} {\"Q2\":>12} {\"Q3\":>12} {\"Q4 (Spicy)\":>12} {\"ALL\":>12} {\"Q1-Q4 Delta\":>15}')\n",
    "            print(f'  {\"-\"*110}')\n",
    "            \n",
    "            for target in targets_to_show:\n",
    "                probs = {}\n",
    "                for regime in ['Q1_Quiet', 'Q2_Neutral', 'Q3_Neutral', 'Q4_Spicy', 'ALL']:\n",
    "                    if regime in regime_results:\n",
    "                        df_r = regime_results[regime]\n",
    "                        row = df_r[df_r['target'] == target]\n",
    "                        if len(row) > 0:\n",
    "                            probs[regime] = row.iloc[0]['prob_conditional']\n",
    "                        else:\n",
    "                            probs[regime] = np.nan\n",
    "                    else:\n",
    "                        probs[regime] = np.nan\n",
    "                \n",
    "                # Calculate delta Q1 - Q4\n",
    "                if not np.isnan(probs.get('Q1_Quiet', np.nan)) and not np.isnan(probs.get('Q4_Spicy', np.nan)):\n",
    "                    delta = probs['Q1_Quiet'] - probs['Q4_Spicy']\n",
    "                else:\n",
    "                    delta = np.nan\n",
    "                \n",
    "                # Format output\n",
    "                q1_str = f\"{probs.get('Q1_Quiet', np.nan):.0%}\" if not np.isnan(probs.get('Q1_Quiet', np.nan)) else \"N/A\"\n",
    "                q2_str = f\"{probs.get('Q2_Neutral', np.nan):.0%}\" if not np.isnan(probs.get('Q2_Neutral', np.nan)) else \"N/A\"\n",
    "                q3_str = f\"{probs.get('Q3_Neutral', np.nan):.0%}\" if not np.isnan(probs.get('Q3_Neutral', np.nan)) else \"N/A\"\n",
    "                q4_str = f\"{probs.get('Q4_Spicy', np.nan):.0%}\" if not np.isnan(probs.get('Q4_Spicy', np.nan)) else \"N/A\"\n",
    "                all_str = f\"{probs.get('ALL', np.nan):.0%}\" if not np.isnan(probs.get('ALL', np.nan)) else \"N/A\"\n",
    "                delta_str = f\"{delta:+.0%}\" if not np.isnan(delta) else \"N/A\"\n",
    "                \n",
    "                print(f\"  {target:20} {q1_str:>12} {q2_str:>12} {q3_str:>12} {q4_str:>12} {all_str:>12} {delta_str:>15}\")\n",
    "            \n",
    "            # Show sample sizes\n",
    "            print(f'\\n  Sample sizes (N days where condition reached):')\n",
    "            for regime in ['Q1_Quiet', 'Q2_Neutral', 'Q3_Neutral', 'Q4_Spicy', 'ALL']:\n",
    "                if regime in regime_results:\n",
    "                    n = regime_results[regime].iloc[0]['n_condition'] if len(regime_results[regime]) > 0 else 0\n",
    "                    print(f'    {regime:12} N = {n}')\n",
    "\n",
    "print(f'\\n{\"=\"*100}')\n",
    "print('[OK] Regime comparison complete')\n",
    "print(f'{\"=\"*100}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Interpretation - Does Regime Matter?\n",
    "\n",
    "**How to read the results:**\n",
    "\n",
    "1. **Q1-Q4 Delta = POSITIVE (e.g., +20%):**\n",
    "   - In quiet markets (Q1), this target is reached MORE often after condition\n",
    "   - Example: P(LPP | LR1) higher in Q1 → stronger mean reversion in quiet markets\n",
    "   - **Trading implication:** Fade breakouts in quiet markets\n",
    "\n",
    "2. **Q1-Q4 Delta = NEGATIVE (e.g., -15%):**\n",
    "   - In spicy markets (Q4), this target is reached MORE often after condition\n",
    "   - Example: P(LR2 | LR1) higher in Q4 → stronger momentum in spicy markets\n",
    "   - **Trading implication:** Follow breakouts in spicy markets\n",
    "\n",
    "3. **Q1-Q4 Delta = SMALL (e.g., ±5%):**\n",
    "   - Regime doesn't significantly affect this probability\n",
    "   - Use the ALL (combined) probability for trading decisions\n",
    "\n",
    "**Key Patterns to Look For:**\n",
    "- **Mean reversion stronger in Q1:** P(reversion_target | breakout_level) higher in Q1\n",
    "- **Momentum stronger in Q4:** P(continuation_target | breakout_level) higher in Q4\n",
    "- **Symmetric across regimes:** No delta → use regime-agnostic probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP 9] Interpretation Guide\n",
      "================================================================================\n",
      "\n",
      "HOW TO USE THESE RESULTS:\n",
      "\n",
      "1. IDENTIFY REGIME-SPECIFIC EDGES:\n",
      "   - Look for large Q1-Q4 deltas (>15%)\n",
      "   - Positive delta → Q1 (Quiet) has higher probability\n",
      "   - Negative delta → Q4 (Spicy) has higher probability\n",
      "\n",
      "2. TRADING APPLICATIONS:\n",
      "\n",
      "   Example A: Mean Reversion Edge in Quiet Markets\n",
      "   --------------------------------------------------\n",
      "   Zone: LPP_LR1, Condition: LR1\n",
      "   Target: LPP (mean reversion)\n",
      "   Q1: 75%, Q4: 45%, Delta: +30%\n",
      "\n",
      "   → In quiet markets (Q1), after hitting LR1, price reverts to LPP 75% of the time\n",
      "   → In spicy markets (Q4), only 45% reversion rate\n",
      "   → TRADE: Fade LR1 breakouts in Q1, avoid fading in Q4\n",
      "\n",
      "   Example B: Momentum Edge in Spicy Markets\n",
      "   --------------------------------------------------\n",
      "   Zone: LPP_LR1, Condition: LR1\n",
      "   Target: LR2 (continuation)\n",
      "   Q1: 25%, Q4: 55%, Delta: -30%\n",
      "\n",
      "   → In quiet markets (Q1), only 25% continuation to LR2\n",
      "   → In spicy markets (Q4), 55% continuation rate\n",
      "   → TRADE: Follow LR1 breakouts in Q4, avoid chasing in Q1\n",
      "\n",
      "3. WHEN TO IGNORE REGIME:\n",
      "   - If Q1-Q4 delta < 10% → regime doesn't matter much\n",
      "   - Use the ALL (combined) probability for trading decisions\n",
      "   - Focus on temporal + spatial conditional probabilities only\n",
      "\n",
      "4. SAMPLE SIZE CHECK:\n",
      "   - Only trust results with N >= 20 days per regime\n",
      "   - If sample size too small, results may be noise\n",
      "   - Combine Q2+Q3 if needed to increase sample size\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print('\\n[STEP 9] Interpretation Guide')\n",
    "print('='*80)\n",
    "print('''\n",
    "HOW TO USE THESE RESULTS:\n",
    "\n",
    "1. IDENTIFY REGIME-SPECIFIC EDGES:\n",
    "   - Look for large Q1-Q4 deltas (>15%)\n",
    "   - Positive delta → Q1 (Quiet) has higher probability\n",
    "   - Negative delta → Q4 (Spicy) has higher probability\n",
    "\n",
    "2. TRADING APPLICATIONS:\n",
    "   \n",
    "   Example A: Mean Reversion Edge in Quiet Markets\n",
    "   --------------------------------------------------\n",
    "   Zone: LPP_LR1, Condition: LR1\n",
    "   Target: LPP (mean reversion)\n",
    "   Q1: 75%, Q4: 45%, Delta: +30%\n",
    "   \n",
    "   → In quiet markets (Q1), after hitting LR1, price reverts to LPP 75% of the time\n",
    "   → In spicy markets (Q4), only 45% reversion rate\n",
    "   → TRADE: Fade LR1 breakouts in Q1, avoid fading in Q4\n",
    "   \n",
    "   Example B: Momentum Edge in Spicy Markets\n",
    "   --------------------------------------------------\n",
    "   Zone: LPP_LR1, Condition: LR1\n",
    "   Target: LR2 (continuation)\n",
    "   Q1: 25%, Q4: 55%, Delta: -30%\n",
    "   \n",
    "   → In quiet markets (Q1), only 25% continuation to LR2\n",
    "   → In spicy markets (Q4), 55% continuation rate\n",
    "   → TRADE: Follow LR1 breakouts in Q4, avoid chasing in Q1\n",
    "\n",
    "3. WHEN TO IGNORE REGIME:\n",
    "   - If Q1-Q4 delta < 10% → regime doesn't matter much\n",
    "   - Use the ALL (combined) probability for trading decisions\n",
    "   - Focus on temporal + spatial conditional probabilities only\n",
    "\n",
    "4. SAMPLE SIZE CHECK:\n",
    "   - Only trust results with N >= 20 days per regime\n",
    "   - If sample size too small, results may be noise\n",
    "   - Combine Q2+Q3 if needed to increase sample size\n",
    "''')\n",
    "print('='*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Export Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP 10] Export Summary\n",
      "================================================================================\n",
      "[OK] Summary table: 750 rows\n",
      "\n",
      "Top 15 strongest regime effects (Q1-Q4 delta):\n",
      "\n",
      "MEAN REVERSION EDGES (Q1 > Q4, positive delta):\n",
      "  LS1_LPP      | LPP             → LR3             | Q1: 47%, Q4: 29%, Δ: +18%\n",
      "  LS1_LPP      | LPP             → LR2_LR3_075     | Q1: 50%, Q4: 32%, Δ: +18%\n",
      "  LS1_LPP      | LPP             → LR2_LR3_050     | Q1: 51%, Q4: 37%, Δ: +14%\n",
      "  LPP_LR1      | LPP_LR1_050     → LR2_LR3_075     | Q1: 55%, Q4: 41%, Δ: +14%\n",
      "  LPP_LR1      | LPP_LR1_050     → LR3             | Q1: 49%, Q4: 36%, Δ: +13%\n",
      "  LPP_LR1      | LPP             → LR2_LR3_075     | Q1: 45%, Q4: 33%, Δ: +13%\n",
      "  LS1_LPP      | LPP             → LR2             | Q1: 61%, Q4: 49%, Δ: +12%\n",
      "  LS1_LPP      | LPP             → LR2_LR3_025     | Q1: 56%, Q4: 44%, Δ: +12%\n",
      "  LPP_LR1      | LPP             → LR3             | Q1: 41%, Q4: 29%, Δ: +12%\n",
      "  LPP_LR1      | LR1             → LR2_LR3_075     | Q1: 63%, Q4: 51%, Δ: +12%\n",
      "\n",
      "MOMENTUM EDGES (Q4 > Q1, negative delta):\n",
      "  LPP_LR1      | LR1             → LR1_LR2_025     | Q1: 50%, Q4: 71%, Δ: -21%\n",
      "  LS1_LPP      | LS1             → LS1_LS2_075     | Q1: 43%, Q4: 63%, Δ: -20%\n",
      "  LPP_LR1      | LPP             → LS1_LPP_075     | Q1: 38%, Q4: 55%, Δ: -17%\n",
      "  LS1_LPP      | LS1             → LS1_LS2_050     | Q1: 62%, Q4: 77%, Δ: -15%\n",
      "  LS1_LPP      | LPP             → LPP_LR1_025     | Q1: 55%, Q4: 67%, Δ: -12%\n",
      "  LPP_LR1      | LPP             → LS1_LPP_050     | Q1: 60%, Q4: 71%, Δ: -11%\n",
      "  LPP_LR1      | LR1             → LR1_LR2_050     | Q1: 73%, Q4: 82%, Δ: -9%\n",
      "  LS1_LPP      | LS1_LPP_050     → LS1_LS2_075     | Q1: 68%, Q4: 76%, Δ: -8%\n",
      "  LS1_LPP      | LS1_LPP_050     → LS1_LPP_025     | Q1: 56%, Q4: 63%, Δ: -7%\n",
      "  LS1_LPP      | LS1_LPP_050     → LPP             | Q1: 42%, Q4: 49%, Δ: -7%\n",
      "\n",
      "[COMPLETE] Local Pivot Conditional Probabilities by Regime Analysis Finished\n",
      "[NEXT STEP] Review Q1-Q4 deltas to identify regime-specific edges for backtesting\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print('\\n[STEP 10] Export Summary')\n",
    "print('='*80)\n",
    "\n",
    "export_rows = []\n",
    "\n",
    "for zone, conditions_dict in all_results.items():\n",
    "    for condition, regime_dict in conditions_dict.items():\n",
    "        for regime, df_result in regime_dict.items():\n",
    "            for _, row in df_result.iterrows():\n",
    "                export_rows.append({\n",
    "                    'zone': zone,\n",
    "                    'condition': condition,\n",
    "                    'regime': regime,\n",
    "                    'target': row['target'],\n",
    "                    'prob_conditional': row['prob_conditional'],\n",
    "                    'count_sequential': row['count_sequential'],\n",
    "                    'n_condition': row['n_condition'],\n",
    "                })\n",
    "\n",
    "if len(export_rows) > 0:\n",
    "    df_export = pd.DataFrame(export_rows)\n",
    "    \n",
    "    print(f'[OK] Summary table: {len(df_export)} rows')\n",
    "    \n",
    "    # Find strongest regime-specific edges (largest Q1-Q4 deltas)\n",
    "    # Pivot to compare Q1 vs Q4\n",
    "    df_pivot = df_export.pivot_table(\n",
    "        index=['zone', 'condition', 'target'],\n",
    "        columns='regime',\n",
    "        values='prob_conditional'\n",
    "    ).reset_index()\n",
    "    \n",
    "    if 'Q1_Quiet' in df_pivot.columns and 'Q4_Spicy' in df_pivot.columns:\n",
    "        df_pivot['Q1_Q4_Delta'] = df_pivot['Q1_Quiet'] - df_pivot['Q4_Spicy']\n",
    "        df_pivot = df_pivot.dropna(subset=['Q1_Q4_Delta'])\n",
    "        \n",
    "        print(f'\\nTop 15 strongest regime effects (Q1-Q4 delta):')\n",
    "        print('\\nMEAN REVERSION EDGES (Q1 > Q4, positive delta):')\n",
    "        top_positive = df_pivot.nlargest(10, 'Q1_Q4_Delta')[['zone', 'condition', 'target', 'Q1_Quiet', 'Q4_Spicy', 'Q1_Q4_Delta']]\n",
    "        if len(top_positive) > 0:\n",
    "            for _, row in top_positive.iterrows():\n",
    "                print(f\"  {row['zone']:12} | {row['condition']:15} → {row['target']:15} | Q1: {row['Q1_Quiet']:.0%}, Q4: {row['Q4_Spicy']:.0%}, Δ: {row['Q1_Q4_Delta']:+.0%}\")\n",
    "        \n",
    "        print(f'\\nMOMENTUM EDGES (Q4 > Q1, negative delta):')\n",
    "        top_negative = df_pivot.nsmallest(10, 'Q1_Q4_Delta')[['zone', 'condition', 'target', 'Q1_Quiet', 'Q4_Spicy', 'Q1_Q4_Delta']]\n",
    "        if len(top_negative) > 0:\n",
    "            for _, row in top_negative.iterrows():\n",
    "                print(f\"  {row['zone']:12} | {row['condition']:15} → {row['target']:15} | Q1: {row['Q1_Quiet']:.0%}, Q4: {row['Q4_Spicy']:.0%}, Δ: {row['Q1_Q4_Delta']:+.0%}\")\n",
    "    \n",
    "    # Optionally save\n",
    "    # df_export.to_csv('../../output/local_pivot_conditional_by_regime.csv', index=False)\n",
    "    # print(f'\\n[OK] Saved to output/local_pivot_conditional_by_regime.csv')\n",
    "else:\n",
    "    print('[WARNING] No results to export')\n",
    "\n",
    "print('\\n[COMPLETE] Local Pivot Conditional Probabilities by Regime Analysis Finished')\n",
    "print('[NEXT STEP] Review Q1-Q4 deltas to identify regime-specific edges for backtesting')\n",
    "print('='*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
