{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DE40 Trend and Range Analysis\n",
    "\n",
    "## Objective\n",
    "Analyze the trend characteristics and range behavior of the DAX Index (DE40) to understand market structure, volatility patterns, and mean-reversion vs. momentum tendencies.\n",
    "\n",
    "## Key Metrics\n",
    "- **Hurst Exponent**: Measure of market efficiency (trending vs. mean-reverting)\n",
    "- **Autocorrelation**: Price momentum persistence\n",
    "- **Range Metrics**: Intraday range, volatility clustering\n",
    "- **Gap Analysis**: Data quality and market anomalies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../../')\n",
    "\n",
    "from shared.database_connector import fetch_ohlcv, get_date_range\n",
    "from shared.data_module import process_data\n",
    "from shared.config import SYMBOLS\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "\n",
    "print(\"[OK] Environment setup complete\")\n",
    "print(f\"DE40 Symbol Info: {SYMBOLS['deuidxeur']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Fetching and Gap Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get available date range\n",
    "date_range = get_date_range('deuidxeur', 'h1')\n",
    "print(f\"Available data: {date_range['start']} to {date_range['end']}\")\n",
    "\n",
    "# Use last 12 months for analysis\n",
    "end_date = date_range['end']\n",
    "start_date = end_date - timedelta(days=365)\n",
    "\n",
    "print(f\"\\nAnalysis period: {start_date.date()} to {end_date.date()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch raw data\n",
    "df_raw = fetch_ohlcv(\n",
    "    symbol='deuidxeur',\n",
    "    timeframe='h1',\n",
    "    start_date=start_date,\n",
    "    end_date=end_date\n",
    ")\n",
    "\n",
    "print(f\"Raw data shape: {df_raw.shape}\")\n",
    "print(f\"\\nRaw data info:\")\n",
    "print(df_raw.info())\n",
    "print(f\"\\nFirst 5 rows:\")\n",
    "print(df_raw.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process data (this will handle gaps, outliers, imputation, timezone conversion)\n",
    "df_clean = process_data(\n",
    "    df=df_raw,\n",
    "    symbol='deuidxeur',\n",
    "    timeframe='h1',\n",
    "    local_time=True,  # Convert to Europe/Berlin timezone\n",
    "    exclude_news=False\n",
    ")\n",
    "\n",
    "print(f\"Cleaned data shape: {df_clean.shape}\")\n",
    "print(f\"\\nData after processing:\")\n",
    "print(df_clean.info())\n",
    "print(f\"\\nTimezone: {df_clean.index.tz}\")\n",
    "print(f\"\\nFirst 5 rows (cleaned):\")\n",
    "print(df_clean.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Gap Analysis - Data Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate missing data statistics\n",
    "missing_before = df_raw.isnull().sum()\n",
    "missing_after = df_clean.isnull().sum()\n",
    "\n",
    "print(\"Missing Data Statistics:\")\n",
    "print(f\"\\nBefore Processing (raw, unfiltered):\")\n",
    "print(missing_before)\n",
    "print(f\"\\nAfter Processing (filtered to market hours):\")\n",
    "print(missing_after)\n",
    "\n",
    "# Gap Analysis - Raw Data (ALL timestamps including nights)\n",
    "print(f\"\\n\" + \"=\"*70)\n",
    "print(\"Gap Analysis - RAW DATA (includes night hours, weekends, holidays)\")\n",
    "print(\"=\"*70)\n",
    "expected_candles_raw = (df_raw.index[-1] - df_raw.index[0]).total_seconds() / 3600  # 1-hour bars\n",
    "actual_candles_raw = len(df_raw)\n",
    "gap_percentage_raw = ((expected_candles_raw - actual_candles_raw) / expected_candles_raw * 100)\n",
    "\n",
    "print(f\"Expected candles (continuous, all hours): {expected_candles_raw:.0f}\")\n",
    "print(f\"Actual candles: {actual_candles_raw}\")\n",
    "print(f\"Missing candles: {expected_candles_raw - actual_candles_raw:.0f} ({gap_percentage_raw:.2f}%)\")\n",
    "print(f\"Note: This includes night hours when market is CLOSED - not meaningful for trading\")\n",
    "\n",
    "# Gap Analysis - Clean Data (MARKET HOURS ONLY)\n",
    "print(f\"\\n\" + \"=\"*70)\n",
    "print(\"Gap Analysis - CLEAN DATA (market hours only: 09:00-17:30 Berlin time)\")\n",
    "print(\"=\"*70)\n",
    "expected_candles_clean = (df_clean.index[-1] - df_clean.index[0]).total_seconds() / 3600  # 1-hour bars\n",
    "actual_candles_clean = len(df_clean)\n",
    "gap_percentage_clean = ((expected_candles_clean - actual_candles_clean) / expected_candles_clean * 100) if expected_candles_clean > 0 else 0\n",
    "\n",
    "print(f\"Expected candles (market hours only): {expected_candles_clean:.0f}\")\n",
    "print(f\"Actual candles: {actual_candles_clean}\")\n",
    "print(f\"Missing candles: {expected_candles_clean - actual_candles_clean:.0f} ({gap_percentage_clean:.2f}%)\")\n",
    "print(f\"Data quality during market hours: {100 - gap_percentage_clean:.1f}%\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*70)\n",
    "print(f\"Summary:\")\n",
    "print(f\"  Raw data candles: {actual_candles_raw}\")\n",
    "print(f\"  Filtered candles (market hours): {actual_candles_clean}\")\n",
    "print(f\"  Removed (night hours, weekends, holidays): {actual_candles_raw - actual_candles_clean}\")\n",
    "print(f\"  Data quality during market hours: {100 - gap_percentage_clean:.1f}%\")\n",
    "print(f\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Calculate missing data statistics\nmissing_before = df_raw.isnull().sum()\nmissing_after = df_clean.isnull().sum()\n\nprint(\"Missing Data Statistics:\")\nprint(f\"\\nBefore Processing (raw, unfiltered):\") \nprint(missing_before)\nprint(f\"\\nAfter Processing (filtered to market hours):\") \nprint(missing_after)\n\n# Gap Analysis - Raw Data (ALL timestamps including nights)\nprint(f\"\\n\" + \"=\"*70)\nprint(\"Gap Analysis - RAW DATA (includes night hours, weekends, holidays)\")\nprint(\"=\"*70)\nprint(f\"Note: Raw data contains ALL hours from database (UTC timestamps)\")\nexpected_candles_raw = (df_raw.index[-1] - df_raw.index[0]).total_seconds() / 3600  # 1-hour bars\nactual_candles_raw = len(df_raw)\ngap_percentage_raw = ((expected_candles_raw - actual_candles_raw) / expected_candles_raw * 100)\n\nprint(f\"Expected candles (continuous, all hours): {expected_candles_raw:.0f}\")\nprint(f\"Actual candles: {actual_candles_raw}\")\nprint(f\"Missing candles: {expected_candles_raw - actual_candles_raw:.0f} ({gap_percentage_raw:.2f}%)\")\nprint(f\"Reason: Includes night hours (23:00-09:00 in DAX local time) when market is CLOSED\")\n\n# Gap Analysis - Clean Data (MARKET HOURS ONLY)\nprint(f\"\\n\" + \"=\"*70)\nprint(\"Gap Analysis - CLEAN DATA (market hours only: 09:00-17:30 Berlin time)\")\nprint(\"=\"*70)\nprint(f\"Note: Data is already FILTERED to trading hours\")\nexpected_candles_clean = (df_clean.index[-1] - df_clean.index[0]).total_seconds() / 3600  # 1-hour bars\nactual_candles_clean = len(df_clean)\ngap_percentage_clean = ((expected_candles_clean - actual_candles_clean) / expected_candles_clean * 100) if expected_candles_clean > 0 else 0\n\nprint(f\"Expected candles (continuous, market hours only): {expected_candles_clean:.0f}\")\nprint(f\"Actual candles: {actual_candles_clean}\")\nprint(f\"Missing candles: {expected_candles_clean - actual_candles_clean:.0f} ({gap_percentage_clean:.2f}%)\")\nprint(f\"Data quality during market hours: {100 - gap_percentage_clean:.1f}%\")\nprint(f\"\\nDST Note: Europe/Berlin timezone automatically applies CET (UTC+1) in winter and CEST (UTC+2) in summer\")\n\nprint(f\"\\n\" + \"=\"*70)\nprint(f\"Summary:\")\nprint(f\"  Raw data candles (all hours): {actual_candles_raw}\")\nprint(f\"  Filtered candles (09:00-17:30, weekdays, non-holidays): {actual_candles_clean}\")\nprint(f\"  Removed (night hours, weekends, holidays): {actual_candles_raw - actual_candles_clean}\")\nprint(f\"  Coverage: {(actual_candles_clean/actual_candles_raw)*100:.1f}% of raw data is trading hours\")\nprint(f\"  Data quality during market hours: {100 - gap_percentage_clean:.1f}%\")\nprint(f\"=\"*70)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Hurst Exponent Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_hurst_exponent(price_series, max_lag=1000):\n",
    "    \"\"\"\n",
    "    Calculate Hurst Exponent using Rescaled Range Analysis.\n",
    "    \n",
    "    H = 0.5: Random walk (no trend, no mean reversion)\n",
    "    H > 0.5: Trending market (momentum)\n",
    "    H < 0.5: Mean-reverting market\n",
    "    \"\"\"\n",
    "    lags = range(10, max_lag, 10)\n",
    "    tau = []\n",
    "    \n",
    "    for lag in lags:\n",
    "        # Get log returns\n",
    "        returns = np.log(price_series / price_series.shift(1)).dropna()\n",
    "        \n",
    "        # Mean-adjusted returns\n",
    "        mean_adjusted = returns - returns.mean()\n",
    "        \n",
    "        # Cumulative sum\n",
    "        cumsum = np.cumsum(mean_adjusted[:lag])\n",
    "        \n",
    "        # Range: max - min\n",
    "        range_val = np.max(cumsum) - np.min(cumsum)\n",
    "        \n",
    "        # Standard deviation\n",
    "        std = np.std(returns[:lag], ddof=1)\n",
    "        \n",
    "        if std > 0:\n",
    "            tau.append(range_val / std)\n",
    "    \n",
    "    # Log-log regression to find slope (Hurst exponent)\n",
    "    lags = np.array(list(lags))[:len(tau)]\n",
    "    poly = np.polyfit(np.log(lags), np.log(tau), 1)\n",
    "    hurst = poly[0]\n",
    "    \n",
    "    return hurst, lags, np.array(tau)\n",
    "\n",
    "print(\"Hurst Exponent function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Hurst exponent for close prices\n",
    "hurst, lags, tau = calculate_hurst_exponent(df_clean['close'], max_lag=500)\n",
    "\n",
    "print(f\"\\nHurst Exponent Analysis:\")\n",
    "print(f\"Hurst Exponent: {hurst:.4f}\")\n",
    "\n",
    "if hurst < 0.45:\n",
    "    interpretation = \"Strong Mean-Reversion (H < 0.45)\"\n",
    "elif hurst < 0.5:\n",
    "    interpretation = \"Mild Mean-Reversion (0.45 ≤ H < 0.5)\"\n",
    "elif hurst == 0.5:\n",
    "    interpretation = \"Random Walk (H = 0.5)\"\n",
    "elif hurst < 0.55:\n",
    "    interpretation = \"Mild Momentum/Trend (0.5 < H < 0.55)\"\n",
    "else:\n",
    "    interpretation = \"Strong Trend (H ≥ 0.55)\"\n",
    "\n",
    "print(f\"Interpretation: {interpretation}\")\n",
    "print(f\"\\nImplication for Trading:\")\n",
    "if hurst < 0.5:\n",
    "    print(\"  -> Mean-reversion strategies may be profitable\")\n",
    "    print(\"  -> Momentum-following strategies may underperform\")\n",
    "else:\n",
    "    print(\"  -> Trending/momentum strategies may be profitable\")\n",
    "    print(\"  -> Mean-reversion strategies may underperform\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Hurst exponent calculation\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Log-log plot\n",
    "ax.loglog(lags, tau, 'bo', markersize=6, label='Rescaled Range (tau)')\n",
    "\n",
    "# Fit line\n",
    "fit_tau = np.exp(np.polyfit(np.log(lags), np.log(tau), 1)[0] * np.log(lags) + np.polyfit(np.log(lags), np.log(tau), 1)[1])\n",
    "ax.loglog(lags, fit_tau, 'r--', linewidth=2, label=f'Fit (H = {hurst:.4f})')\n",
    "\n",
    "ax.set_xlabel('Time Lag (hours)', fontsize=11, fontweight='bold')\n",
    "ax.set_ylabel('Rescaled Range (R/S)', fontsize=11, fontweight='bold')\n",
    "ax.set_title(f'Hurst Exponent Analysis - DE40 (H = {hurst:.4f})', fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, which='both', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Autocorrelation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "# Calculate returns\n",
    "returns = np.log(df_clean['close'] / df_clean['close'].shift(1)).dropna()\n",
    "\n",
    "print(f\"Return Statistics:\")\n",
    "print(f\"Mean return: {returns.mean() * 100:.4f}%\")\n",
    "print(f\"Std dev: {returns.std() * 100:.4f}%\")\n",
    "print(f\"Sharpe ratio (annualized): {(returns.mean() / returns.std()) * np.sqrt(252*24):.4f}\")\n",
    "\n",
    "# ADF test for stationarity\n",
    "adf_result = adfuller(returns, autolag='AIC')\n",
    "print(f\"\\nADF Test (Returns):\")\n",
    "print(f\"ADF Statistic: {adf_result[0]:.6f}\")\n",
    "print(f\"P-value: {adf_result[1]:.6f}\")\n",
    "print(f\"Stationary: {'Yes (p < 0.05)' if adf_result[1] < 0.05 else 'No (p >= 0.05)'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ACF/PACF plots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# ACF of returns\n",
    "plot_acf(returns, lags=50, ax=axes[0, 0])\n",
    "axes[0, 0].set_title('ACF - Log Returns (1h bars)', fontsize=11, fontweight='bold')\n",
    "\n",
    "# PACF of returns\n",
    "plot_pacf(returns, lags=50, ax=axes[0, 1])\n",
    "axes[0, 1].set_title('PACF - Log Returns (1h bars)', fontsize=11, fontweight='bold')\n",
    "\n",
    "# ACF of squared returns (volatility clustering)\n",
    "plot_acf(returns**2, lags=50, ax=axes[1, 0])\n",
    "axes[1, 0].set_title('ACF - Squared Returns (Volatility)', fontsize=11, fontweight='bold')\n",
    "\n",
    "# Rolling volatility\n",
    "rolling_vol = returns.rolling(window=24).std() * 100  # 24-hour rolling vol\n",
    "axes[1, 1].plot(rolling_vol.index, rolling_vol.values, linewidth=1, color='steelblue')\n",
    "axes[1, 1].set_title('24h Rolling Volatility', fontsize=11, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Date')\n",
    "axes[1, 1].set_ylabel('Volatility (%)')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"[OK] Autocorrelation analysis complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate autocorrelation at specific lags\n",
    "lags_to_check = [1, 2, 4, 8, 24, 48]\n",
    "autocorr_values = [returns.autocorr(lag=lag) for lag in lags_to_check]\n",
    "\n",
    "print(f\"Autocorrelation at Specific Lags:\")\n",
    "for lag, autocorr in zip(lags_to_check, autocorr_values):\n",
    "    print(f\"  Lag {lag:2d}: {autocorr:7.4f}\")\n",
    "\n",
    "# Interpretation\n",
    "if max(abs(ac) for ac in autocorr_values) < 0.05:\n",
    "    print(\"\\nInterpretation: No significant autocorrelation - random walk behavior\")\n",
    "elif any(ac > 0.1 for ac in autocorr_values):\n",
    "    print(\"\\nInterpretation: Positive autocorrelation - momentum signals may exist\")\n",
    "elif any(ac < -0.1 for ac in autocorr_values):\n",
    "    print(\"\\nInterpretation: Negative autocorrelation - mean reversion signals may exist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Range and Volatility Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate range metrics\n",
    "df_clean['range'] = df_clean['high'] - df_clean['low']\n",
    "df_clean['range_pct'] = (df_clean['range'] / df_clean['open'] * 100)\n",
    "df_clean['hl2'] = (df_clean['high'] + df_clean['low']) / 2\n",
    "df_clean['true_range'] = np.maximum(\n",
    "    df_clean['high'] - df_clean['low'],\n",
    "    np.maximum(\n",
    "        abs(df_clean['high'] - df_clean['close'].shift(1)),\n",
    "        abs(df_clean['low'] - df_clean['close'].shift(1))\n",
    "    )\n",
    ")\n",
    "\n",
    "print(f\"Range Statistics:\")\n",
    "print(f\"Mean range: {df_clean['range'].mean():.2f} points\")\n",
    "print(f\"Median range: {df_clean['range'].median():.2f} points\")\n",
    "print(f\"Std dev range: {df_clean['range'].std():.2f} points\")\n",
    "print(f\"Min range: {df_clean['range'].min():.2f} points\")\n",
    "print(f\"Max range: {df_clean['range'].max():.2f} points\")\n",
    "\n",
    "print(f\"\\nRange % Statistics:\")\n",
    "print(f\"Mean range %: {df_clean['range_pct'].mean():.4f}%\")\n",
    "print(f\"Median range %: {df_clean['range_pct'].median():.4f}%\")\n",
    "print(f\"Std dev range %: {df_clean['range_pct'].std():.4f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize range distribution and time series\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Range histogram\n",
    "axes[0, 0].hist(df_clean['range'], bins=50, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "axes[0, 0].axvline(df_clean['range'].mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {df_clean[\"range\"].mean():.2f}')\n",
    "axes[0, 0].set_xlabel('Range (points)', fontweight='bold')\n",
    "axes[0, 0].set_ylabel('Frequency', fontweight='bold')\n",
    "axes[0, 0].set_title('Range Distribution', fontweight='bold')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Range time series\n",
    "axes[0, 1].plot(df_clean.index, df_clean['range'], linewidth=0.8, color='steelblue', alpha=0.7)\n",
    "axes[0, 1].plot(df_clean.index, df_clean['range'].rolling(24).mean(), linewidth=2, color='red', label='24h MA')\n",
    "axes[0, 1].set_xlabel('Date', fontweight='bold')\n",
    "axes[0, 1].set_ylabel('Range (points)', fontweight='bold')\n",
    "axes[0, 1].set_title('Range Over Time', fontweight='bold')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Range % distribution\n",
    "axes[1, 0].hist(df_clean['range_pct'], bins=50, color='green', edgecolor='black', alpha=0.7)\n",
    "axes[1, 0].axvline(df_clean['range_pct'].mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {df_clean[\"range_pct\"].mean():.4f}%')\n",
    "axes[1, 0].set_xlabel('Range %', fontweight='bold')\n",
    "axes[1, 0].set_ylabel('Frequency', fontweight='bold')\n",
    "axes[1, 0].set_title('Range % Distribution', fontweight='bold')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# ATR (Average True Range)\n",
    "atr = df_clean['true_range'].rolling(14).mean()\n",
    "axes[1, 1].plot(df_clean.index, atr, linewidth=1.5, color='purple', label='ATR(14)')\n",
    "axes[1, 1].fill_between(df_clean.index, atr * 0.5, atr * 1.5, alpha=0.2, color='purple')\n",
    "axes[1, 1].set_xlabel('Date', fontweight='bold')\n",
    "axes[1, 1].set_ylabel('ATR (points)', fontweight='bold')\n",
    "axes[1, 1].set_title('Average True Range (14)', fontweight='bold')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Conclusions and Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"DE40 TREND AND RANGE ANALYSIS - SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\n1. DATA QUALITY:\")\n",
    "print(f\"   * Period Analyzed: {df_clean.index[0].date()} to {df_clean.index[-1].date()}\")\n",
    "print(f\"   * Total Candles: {len(df_clean)}\")\n",
    "print(f\"   * Missing Data: {missing_after.sum()} rows (after imputation: {missing_after.sum() == 0})\")\n",
    "print(f\"   * Gap Assessment: {gap_percentage_clean:.2f}% missing (during market hours)\")\n",
    "\n",
    "print(f\"\\n2. MARKET EFFICIENCY (HURST EXPONENT):\")\n",
    "print(f\"   * H-value: {hurst:.4f}\")\n",
    "print(f\"   * Type: {interpretation}\")\n",
    "print(f\"   * Volatility: Annualized = {returns.std() * np.sqrt(252*24) * 100:.2f}%\")\n",
    "\n",
    "print(f\"\\n3. AUTOCORRELATION:\")\n",
    "for lag, autocorr in zip(lags_to_check, autocorr_values):\n",
    "    significance = \"***\" if abs(autocorr) > 0.1 else \"*\" if abs(autocorr) > 0.05 else \"\"\n",
    "    print(f\"   * Lag {lag:2d}: {autocorr:7.4f} {significance}\")\n",
    "\n",
    "print(f\"\\n4. VOLATILITY & RANGE:\")\n",
    "print(f\"   * Mean Range: {df_clean['range'].mean():.2f} points ({df_clean['range_pct'].mean():.4f}%)\")\n",
    "print(f\"   * Range Std Dev: {df_clean['range'].std():.2f} points\")\n",
    "print(f\"   * ATR(14): {atr.iloc[-1]:.2f} points\")\n",
    "print(f\"   * Volatility Clustering: {'Yes' if any(abs(ac) > 0.1 for ac in autocorr_values[:4]) else 'No significant'}\")\n",
    "\n",
    "print(f\"\\n5. TRADING IMPLICATIONS:\")\n",
    "if hurst < 0.5:\n",
    "    print(f\"   -> Mean-reversion strategies may work (H < 0.5)\")\n",
    "    print(f\"   -> Implement range-trading on intraday levels\")\n",
    "else:\n",
    "    print(f\"   -> Trending/momentum strategies may work (H > 0.5)\")\n",
    "    print(f\"   -> Follow breakouts and maintain directional bias\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}